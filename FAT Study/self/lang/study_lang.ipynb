{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9954522a-1d31-499c-becd-85402580b73e",
   "metadata": {},
   "source": [
    "## REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76f7abd5-fd70-4870-9e82-8fa1fe35431b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Email: ['john.doe@example.com']\n",
      "Extracted Phone: ['+1-123-456-7890']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"Contact me at john.doe@example.com or call me at +1-123-456-7890.\"\n",
    "\n",
    "# Extract email\n",
    "email = re.findall(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', text)\n",
    "\n",
    "# Extract phone number\n",
    "phone = re.findall(r'\\+?\\d{1,4}[-.\\s]?\\(?\\d{1,4}\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}', text)\n",
    "\n",
    "print(\"Extracted Email:\", email)\n",
    "print(\"Extracted Phone:\", phone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51533f12-46a7-4e3e-8ade-ae298bda02db",
   "metadata": {},
   "source": [
    "## Spacy Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1533fca9-c413-48b9-9414-237526b5c4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['John', 'works', 'at', 'OpenAI', '.', 'His', 'email', 'is', 'john.doe@example.com', '.']\n",
      "Lemmas: ['John', 'work', 'at', 'OpenAI', '.', 'his', 'email', 'be', 'john.doe@example.com', '.']\n",
      "POS Tags: [('John', 'PROPN'), ('works', 'VERB'), ('at', 'ADP'), ('OpenAI', 'PROPN'), ('.', 'PUNCT'), ('His', 'PRON'), ('email', 'NOUN'), ('is', 'AUX'), ('john.doe@example.com', 'PROPN'), ('.', 'PUNCT')]\n",
      "Entities: [('John', 'PERSON')]\n",
      "Without Stopwords: ['John', 'works', 'OpenAI', '.', 'email', 'john.doe@example.com', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"John works at OpenAI. His email is john.doe@example.com.\")\n",
    "\n",
    "# Tokenization\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "# Lemmatization\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "# Named Entity Recognition (NER)\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Stopword Removal\n",
    "stopwords = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Lemmas:\", lemmas)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "print(\"Entities:\", entities)\n",
    "print(\"Without Stopwords:\", stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218b5947-8acd-44ef-be4d-1a28272bb7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['john', 'work', 'at', 'openai', '.', 'hi', 'email', 'is', 'john.doe@example.com', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "## Stemming\n",
    "ps = PorterStemmer()\n",
    "stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11520cc0-fd4d-4688-96b8-5d915f2ba040",
   "metadata": {},
   "source": [
    "## TF-IDF, Cosine Similarity, and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6104b5de-57f7-4fe6-aa18-6fdd8f0aa97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: [[0.37620501 0.71942228]]\n",
      "Accuracy: 0.9723618090452262\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Sample data\n",
    "corpus = [\"The sky is blue.\", \"The sun is bright.\", \"The sun in the blue sky is bright.\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Cosine Similarity\n",
    "cos_sim = cosine_similarity(X[0], X[1:])\n",
    "print(\"Cosine Similarity:\", cos_sim)\n",
    "\n",
    "# Classification Example\n",
    "data = fetch_20newsgroups(subset='all', categories=['rec.sport.hockey', 'sci.space'], remove=('headers', 'footers', 'quotes'))\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vec, y_train)\n",
    "print(\"Accuracy:\", clf.score(vectorizer.transform(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7be568b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Vocabulary: ['and this' 'document is' 'first document' 'is the' 'is this'\n",
      " 'second document' 'the first' 'the second' 'the third' 'third one'\n",
      " 'this document' 'this is' 'this the']\n",
      "TF-IDF Vocabulary: ['and this' 'document is' 'first document' 'is the' 'is this'\n",
      " 'second document' 'the first' 'the second' 'the third' 'third one'\n",
      " 'this document' 'this is' 'this the']\n",
      "\n",
      "Bag of Words Vectors (Dense):\n",
      " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      "\n",
      "TF-IDF Vectors (Dense):\n",
      " [[0.         0.         0.52303503 0.42344193 0.         0.\n",
      "  0.52303503 0.         0.         0.         0.         0.52303503\n",
      "  0.        ]\n",
      " [0.         0.47633035 0.         0.30403549 0.         0.47633035\n",
      "  0.         0.47633035 0.         0.         0.47633035 0.\n",
      "  0.        ]\n",
      " [0.49819711 0.         0.         0.31799276 0.         0.\n",
      "  0.         0.         0.49819711 0.49819711 0.         0.39278432\n",
      "  0.        ]\n",
      " [0.         0.         0.43779123 0.         0.55528266 0.\n",
      "  0.43779123 0.         0.         0.         0.         0.\n",
      "  0.55528266]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Sample text data\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer for Bag of Words (with unigrams and bigrams)\n",
    "bow_vectorizer = CountVectorizer(ngram_range=(2, 2))  # bigrams\n",
    "bow_vectors = bow_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Initialize TfidfVectorizer for TF-IDF (with unigrams and bigrams)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2, 2))  # bigrams\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Display the features (vocabulary)\n",
    "print(\"Bag of Words Vocabulary:\", bow_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display vectorized results as arrays (convert sparse matrix to dense)\n",
    "print(\"\\nBag of Words Vectors (Dense):\\n\", bow_vectors.toarray())\n",
    "print(\"\\nTF-IDF Vectors (Dense):\\n\", tfidf_vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0dae3-0035-41d5-877b-09a49d703b47",
   "metadata": {},
   "source": [
    "## Word2Vec: Real vs. Fake Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f3f2247-30b7-4589-b2c2-e64ecede0beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8978494623655914\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load pretrained GloVe embeddings\n",
    "glove = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "data = pd.read_csv(\"spam.csv\")\n",
    "\n",
    "texts = data.Message\n",
    "labels = [0 if i == \"spam\" else 1 for i in data.Category]\n",
    "\n",
    "# Sample data\n",
    "# texts = [\"This is a real message.\", \"Congratulations, you have won!\", \"Claim your prize now.\"]\n",
    "# labels = [1, 0, 0]  # Real: 1, Fake: 0\n",
    "\n",
    "# # Vectorizing text using GloVe embeddings\n",
    "def vectorize(text):\n",
    "    words = text.split()\n",
    "    vectors = [glove[word] for word in words if word in glove]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(50)\n",
    "\n",
    "X = np.array([vectorize(text) for text in texts])\n",
    "# X_train, y_train = X, labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# # Logistic Regression for Classification\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6c741d-fdc7-4e0b-8a94-e2700f53eb6f",
   "metadata": {},
   "source": [
    "## GloVe: Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c492a901-71ae-45a1-bc24-051c898c9daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector for Computer\n",
      "[-0.00515624 -0.00666834 -0.00777684  0.00831073 -0.00198234 -0.00685496\n",
      " -0.00415439  0.00514413 -0.00286914 -0.00374966  0.00162143 -0.00277629\n",
      " -0.00158436  0.00107449 -0.00297794  0.00851928  0.00391094 -0.00995886\n",
      "  0.0062596  -0.00675425  0.00076943  0.00440423 -0.00510337 -0.00211067\n",
      "  0.00809548 -0.00424379 -0.00763626  0.00925791 -0.0021555  -0.00471943\n",
      "  0.0085708   0.00428334  0.00432484  0.00928451 -0.00845308  0.00525532\n",
      "  0.00203935  0.00418828  0.0016979   0.00446413  0.00448629  0.00610452\n",
      " -0.0032021  -0.00457573 -0.00042652  0.00253373 -0.00326317  0.00605772\n",
      "  0.00415413  0.00776459  0.00256927  0.00811668 -0.00138721  0.00807793\n",
      "  0.00371702 -0.00804732 -0.00393361 -0.00247188  0.00489304 -0.00087216\n",
      " -0.00283091  0.00783371  0.0093229  -0.00161493 -0.00515925 -0.00470176\n",
      " -0.00484605 -0.00960283  0.00137202 -0.00422492  0.00252671  0.00561448\n",
      " -0.00406591 -0.00959658  0.0015467  -0.00670012  0.00249517 -0.00378063\n",
      "  0.00707842  0.00064022  0.00356094 -0.00273913 -0.00171055  0.00765279\n",
      "  0.00140768 -0.00585045 -0.0078345   0.00123269  0.00645463  0.00555635\n",
      " -0.00897705  0.00859216  0.00404698  0.00746961  0.00974633 -0.00728958\n",
      " -0.00903996  0.005836    0.00939121  0.00350693]\n",
      "[('system', 0.21617141366004944), ('survey', 0.04466388002038002), ('interface', 0.015025208704173565), ('time', 0.0019419174641370773), ('trees', -0.03284316137433052), ('human', -0.0742427185177803), ('response', -0.09332174807786942), ('graph', -0.09575342386960983), ('eps', -0.10513807833194733), ('user', -0.1693524569272995)]\n",
      "0.0019419217\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "model = Word2Vec(\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    ")\n",
    "model.build_vocab(common_texts, progress_per=1000)\n",
    "model.train(common_texts, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "vector = model.wv['computer'] \n",
    "\n",
    "print(\"Word vector for Computer\")\n",
    "print(vector)\n",
    "\n",
    "sims = model.wv.most_similar('computer', topn=10)  # get other similar words\n",
    "print(sims)\n",
    "print(model.wv.similarity(w1=\"computer\", w2=\"time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4435e-2da1-4426-840d-c85ae11ff644",
   "metadata": {},
   "source": [
    "## LSTM/GRU for Sentiment Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d30c7d84-8ff4-4281-9b70-3277730f60f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8595 - loss: 0.4206\n",
      "Epoch 2/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9820 - loss: 0.0753\n",
      "Epoch 3/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9926 - loss: 0.0290\n",
      "Epoch 4/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9949 - loss: 0.0200\n",
      "Epoch 5/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9985 - loss: 0.0080\n",
      "Epoch 6/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9986 - loss: 0.0056\n",
      "Epoch 7/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9997 - loss: 0.0027\n",
      "Epoch 8/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 0.0016\n",
      "Epoch 9/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 10/10\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.6701e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ba326035c0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# Sample data\n",
    "texts = data.Message\n",
    "labels = [0 if i == \"spam\" else 1 for i in data.Category]\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=5)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=50),\n",
    "    LSTM(50),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e8295-eb95-4900-ab7e-345e604e8bcc",
   "metadata": {},
   "source": [
    "## Transformers - Sent Analysis, QA, Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "579ce00b-a1b4-4364-a1da-02b11bf526fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9971315860748291}\n",
      "NLP tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 20, but your input_length is only 15. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Transformers provide a wide variety of NLP capabilities. Hugging Face\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Sentiment Analysis\n",
    "classifier = pipeline(\"sentiment-analysis\",  model=\"distilbert-base-uncased-finetuned-sst-2-english\", framework=\"pt\")\n",
    "result = classifier(\"I love using Hugging Face transformers!\")\n",
    "print(result[0])\n",
    "\n",
    "# Question Answering\n",
    "qa = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\", framework=\"pt\")\n",
    "context = \"Transformers are state-of-the-art models for NLP tasks.\"\n",
    "result = qa(question=\"What are Transformers used for?\", context=context)\n",
    "print(result[\"answer\"])\n",
    "\n",
    "# Text Summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", framework=\"pt\")\n",
    "text = \"Hugging Face Transformers provide a wide variety of NLP capabilities.\"\n",
    "summary = summarizer(text, max_length=20, min_length=5, do_sample=False)\n",
    "print(summary[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ab3bb-2ec8-4e9b-9fd8-c850d0330bfc",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53845e17-1578-4111-913b-5a435b31edeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 60\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     62\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Sample data (replace with your dataset)\n",
    "texts = data.Message.tolist()  # Example: List of messages\n",
    "labels = [0 if i == \"spam\" else 1 for i in data.Category]  # Convert categories to 0 and 1\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define a custom dataset class for tokenization\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = SpamDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = SpamDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Set up the optimizer and device\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for input_ids, attention_mask, labels in train_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in test_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3d0b51-e613-4598-9626-4a2f86510f08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
