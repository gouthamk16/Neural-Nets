{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":791838,"sourceType":"datasetVersion","datasetId":1895}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## DA5 - Text Summarization using BERT\nGoutham Krishnan 21BAI1007","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom torch import cuda\n\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-19T14:15:37.592980Z","iopub.execute_input":"2024-11-19T14:15:37.593259Z","iopub.status.idle":"2024-11-19T14:15:43.851840Z","shell.execute_reply.started":"2024-11-19T14:15:37.593234Z","shell.execute_reply":"2024-11-19T14:15:43.850894Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class NewsDataset(Dataset):\n    \n    def __init__(self, dataframe, tokenizer, input_length, summary_length):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.input_length = input_length\n        self.summary_length = summary_length\n        self.summary = self.data.text\n        self.article = self.data.ctext\n\n    def __len__(self):\n        return len(self.summary)\n\n    def __getitem__(self, idx):\n        article = str(self.article[idx])\n        article = ' '.join(article.split())\n\n        summary = str(self.summary[idx])\n        summary = ' '.join(summary.split())\n\n        source = self.tokenizer.batch_encode_plus(\n            [article], \n            max_length=self.input_length, \n            pad_to_max_length=True,\n            return_tensors='pt'\n        )\n        \n        target = self.tokenizer.batch_encode_plus(\n            [summary], \n            max_length=self.summary_length, \n            pad_to_max_length=True,\n            return_tensors='pt'\n        )\n\n        source_ids = source['input_ids'].squeeze()\n        source_mask = source['attention_mask'].squeeze()\n        target_ids = target['input_ids'].squeeze()\n        target_mask = target['attention_mask'].squeeze()\n\n        return {\n            'source_ids': source_ids.to(dtype=torch.long), \n            'source_mask': source_mask.to(dtype=torch.long), \n            'target_ids': target_ids.to(dtype=torch.long),\n            'target_ids_y': target_ids.to(dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-11-19T14:16:13.584533Z","iopub.execute_input":"2024-11-19T14:16:13.585049Z","iopub.status.idle":"2024-11-19T14:16:13.593380Z","shell.execute_reply.started":"2024-11-19T14:16:13.584986Z","shell.execute_reply":"2024-11-19T14:16:13.592401Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def train_epoch(epoch, tokenizer, model, device, loader, optimizer):\n    model.train()\n    for batch_idx, batch in enumerate(loader, 0):\n        target_ids = batch['target_ids'].to(device, dtype=torch.long)\n        input_ids = batch['source_ids'].to(device, dtype=torch.long)\n        attention_mask = batch['source_mask'].to(device, dtype=torch.long)\n        \n        decoder_input = target_ids[:, :-1].contiguous()\n        \n        labels = target_ids[:, 1:].clone().detach()\n        labels[target_ids[:, 1:] == tokenizer.pad_token_id] = -100\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input,\n            labels=labels\n        )\n        loss = outputs[0]\n        \n        if batch_idx % 500 == 0:\n            print(f'Epoch: {epoch}, Loss: {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-11-19T14:16:46.544906Z","iopub.execute_input":"2024-11-19T14:16:46.545717Z","iopub.status.idle":"2024-11-19T14:16:46.552067Z","shell.execute_reply.started":"2024-11-19T14:16:46.545683Z","shell.execute_reply":"2024-11-19T14:16:46.551263Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def generate_summaries(epoch, tokenizer, model, device, loader):\n    model.eval()\n    generated_summaries = []\n    reference_summaries = []\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(loader, 0):\n            target_ids = batch['target_ids'].to(device, dtype=torch.long)\n            input_ids = batch['source_ids'].to(device, dtype=torch.long)\n            attention_mask = batch['source_mask'].to(device, dtype=torch.long)\n\n            generated_ids = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=150,\n                num_beams=2,\n                repetition_penalty=2.5,\n                length_penalty=1.0,\n                early_stopping=True\n            )\n            \n            predictions = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) \n                         for g in generated_ids]\n            actual = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n                     for t in target_ids]\n            \n            if batch_idx % 100 == 0:\n                print(f'Processed {batch_idx} batches')\n\n            generated_summaries.extend(predictions)\n            reference_summaries.extend(actual)\n            \n    return generated_summaries, reference_summaries","metadata":{"execution":{"iopub.status.busy":"2024-11-19T14:16:48.151395Z","iopub.execute_input":"2024-11-19T14:16:48.151731Z","iopub.status.idle":"2024-11-19T14:16:48.158922Z","shell.execute_reply.started":"2024-11-19T14:16:48.151703Z","shell.execute_reply":"2024-11-19T14:16:48.157915Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\n    'train_batch_size': 2,\n    'valid_batch_size': 2,\n    'train_epochs': 2,\n    'val_epochs': 1,\n    'learning_rate': 1e-4,\n    'max_input_length': 512,\n    'max_summary_length': 150,\n    'random_seed': 42\n}\n\ntorch.manual_seed(CONFIG['random_seed'])\nnp.random.seed(CONFIG['random_seed'])\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2024-11-19T14:17:20.137605Z","iopub.execute_input":"2024-11-19T14:17:20.138416Z","iopub.status.idle":"2024-11-19T14:17:20.147280Z","shell.execute_reply.started":"2024-11-19T14:17:20.138367Z","shell.execute_reply":"2024-11-19T14:17:20.146531Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Initialize tokenizer\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n    \n# Load and preprocess data\ndf = pd.read_csv('/kaggle/input/news-summary/news_summary.csv', encoding='latin-1')\ndf = df[['text', 'ctext']]\ndf.ctext = 'summarize: ' + df.ctext\n    \n# Split data\ntrain_size = 0.8\ntrain_data = df.sample(frac=train_size, random_state=CONFIG['random_seed']).reset_index(drop=True)\nval_data = df.drop(train_data.index).reset_index(drop=True)\n    \n# Create datasets\ntrain_dataset = NewsDataset(train_data, tokenizer, CONFIG['max_input_length'], CONFIG['max_summary_length'])\nval_dataset = NewsDataset(val_data, tokenizer, CONFIG['max_input_length'], CONFIG['max_summary_length'])\n    \n# Create dataloaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=CONFIG['train_batch_size'],\n    shuffle=True,\n    num_workers=0\n)\n    \nval_loader = DataLoader(\n    val_dataset,\n    batch_size=CONFIG['valid_batch_size'],\n    shuffle=False,\n    num_workers=0\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T14:17:58.473357Z","iopub.execute_input":"2024-11-19T14:17:58.474043Z","iopub.status.idle":"2024-11-19T14:18:00.372840Z","shell.execute_reply.started":"2024-11-19T14:17:58.474012Z","shell.execute_reply":"2024-11-19T14:18:00.372130Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"237021ef75c44cb7b2f48ae96f68419c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0be5212480d4caf9b3e965627176e1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20913dc68c464f0cbed0dbb396f72335"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n    \n# Training loop\nprint('Starting model fine-tuning...')\nfor epoch in range(CONFIG['train_epochs']):\n    train_epoch(epoch, tokenizer, model, device, train_loader, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T14:18:59.247142Z","iopub.execute_input":"2024-11-19T14:18:59.247946Z","iopub.status.idle":"2024-11-19T14:32:35.242775Z","shell.execute_reply.started":"2024-11-19T14:18:59.247915Z","shell.execute_reply":"2024-11-19T14:32:35.242043Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e69c22e0a55a4297af50a713574bcbf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2756ca871d44ad7a1231e9147377e86"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Starting model fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2837: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Loss: 7.238336086273193\nEpoch: 0, Loss: 1.5305217504501343\nEpoch: 0, Loss: 1.5214146375656128\nEpoch: 0, Loss: 1.441329002380371\nEpoch: 1, Loss: 2.829712390899658\nEpoch: 1, Loss: 1.1933395862579346\nEpoch: 1, Loss: 1.338054895401001\nEpoch: 1, Loss: 1.1567189693450928\n","output_type":"stream"}]},{"cell_type":"code","source":"## Save the model\ntorch.save(model.state_dict(), \"summarization_mode.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-11-19T14:38:56.468082Z","iopub.execute_input":"2024-11-19T14:38:56.468905Z","iopub.status.idle":"2024-11-19T14:38:57.605809Z","shell.execute_reply.started":"2024-11-19T14:38:56.468870Z","shell.execute_reply":"2024-11-19T14:38:57.604815Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"print('Generating summaries for validation set...')\nfor epoch in range(CONFIG['val_epochs']):\n    predictions, actuals = generate_summaries(epoch, tokenizer, model, device, val_loader)\n    results_df = pd.DataFrame({'Generated Text': predictions, 'Actual Text': actuals})\n    results_df.to_csv('predictions.csv')\n    print('Results saved to predictions.csv')","metadata":{"execution":{"iopub.status.busy":"2024-11-19T14:39:26.751832Z","iopub.execute_input":"2024-11-19T14:39:26.752546Z","iopub.status.idle":"2024-11-19T14:51:20.064593Z","shell.execute_reply.started":"2024-11-19T14:39:26.752515Z","shell.execute_reply":"2024-11-19T14:51:20.063656Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Generating summaries for validation set...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2837: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Processed 0 batches\nProcessed 100 batches\nProcessed 200 batches\nProcessed 300 batches\nProcessed 400 batches\nResults saved to predictions.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Generate sample summaries","metadata":{}},{"cell_type":"code","source":"for i in range(10):\n    print(f\"Text:\\n{results_df.loc[i, 'Actual Text']}\")\n    print(f\"Summary:\\n{results_df.loc[i, 'Generated Text']}\")\n    print() ","metadata":{"execution":{"iopub.status.busy":"2024-11-19T14:57:25.629909Z","iopub.execute_input":"2024-11-19T14:57:25.630539Z","iopub.status.idle":"2024-11-19T14:57:25.636130Z","shell.execute_reply.started":"2024-11-19T14:57:25.630504Z","shell.execute_reply":"2024-11-19T14:57:25.635220Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Text:\nPM Narendra Modi on Thursday launched Ude Desh ka Aam Nagrik (UDAN) scheme for regional flight connectivity by flagging off the inaugural flight from Shimla to Delhi. Under UDAN, government will connect small towns by air with 50% plane seats' fare capped at?2,500 for a one-hour journey of 500 kilometres. UDAN will connect over 45 unserved and under-served airports.\nSummary:\nthe first UDAN flight took off from Shimla on Monday after being flagged off by Prime Minister Narendra Modi. The flight will be operated by Alliance Air, the regional arm of Air India. The scheme seeks to make flying more affordable for the common people.\n\nText:\nBJP chief Amit Shah on Wednesday slammed Delhi Chief Minister Arvind Kejriwal after AAP blamed EVMs for their loss in the MCD polls. \"Let Kejriwal first tell the public how he won in Delhi in 2015 with these same EVMs...\" Shah said. \"UPA one and two had also won elections with the same EVMs,\" the BJP leader added.\nSummary:\nBJP chief Amit Shah on Wednesday slammed Delhi CM Arvind Kejriwal over allegations of faulty electronic voting machines. \"If EVM machines are faulty then how did they win earlier? I think they should accept the verdict with the right spirit,\" he added.\n\nText:\nSujal Kahar, a 12-year-old football enthusiast and goalkeeper from Mumbai, has been shortlisted to represent India at the global Football For Friendship (F4F) social programme. It will be held in Russia's St Petersburg in July. India will join the F4F programme for the first time and will participate alongside over 60 other countries.\nSummary:\nEarlier this month, the government announced that it will not be taking any further steps to improve the quality of life of people in India. \"It's important for us to know that we are doing well,\" said an official.\n\nText:\nEight member nations voted against BCCI's proposal of retaining ICC's old revenue model at the Board meeting in Dubai on Wednesday. The Sri Lankan board was the only member which supported BCCI's stance on governance, while it joined others in opposing BCCI's proposal to retain the revenue structure. BCCI will lose?1,000 crore if the new revenue model is implemented.\nSummary:\nBCCI lost the vote on 'governance and constitutional changes' by a 1-9 margin while the revenue model saw India getting walloped by a 2-8 margin. The only country that voted alongside BCCI was Sri Lanka.\n\nText:\nFormer AAP leader Mayank Gandhi urged Delhi CM Arvind Kejriwal in an open letter to forget his ambitions of becoming Prime Minister and focus on governing the national capital instead. \"You took complete power by becoming convener as well as CM of Delhi. How much more power do you want before you start delivering alternative politics?\" he added.\nSummary:\nformer Aam Aadmi Party leader Mayank Gandhi has slammed Delhi CM Arvind Kejriwal in an open letter. \"A defeat deals a body blow to arrogance and makes the mind ready to reflect,\" Gandhi wrote. The letter, which was posted on his blog and tweeted from his handle, addressed Kejriwal's place as a politician who wanted to manipulate that support to fuel his ambition to become a PM in 2019.\n\nText:\nAt least three people were killed and several injured after a jetty collapsed due to high tide in West Bengal's Hooghly district on Wednesday. Reports claimed that several people survived by swimming towards the river's bank, while at least 30 others were missing. CM Mamata Banerjee announced a compensation of?2 lakh each to the families of the deceased.\nSummary:\na jetty wrecked in Hooghly district of West Bengal on Wednesday, killing three people and injuring several others. While several people survived by swimming towards the bank of the river, three of them died and many others were injured. Questions are being raised on why the jetty and boat services were not alerted about high tide during certain hours of the day.\n\nText:\nManjit Kaur, a Sub-Inspector in the Punjab Police, recently married her same-sex partner, according to reports. The marriage was reportedly solemnised according to Hindu rituals in Jalandhar and the cop wore a red turban and her bride wore a lehenga. The reports further added that the wedding took place in the presence of their families and colleagues.\nSummary:\nManjit Kaur, a sub-inspector in the Punjab Police, married a woman and it was solemnised according to Hindu rituals. While the cop wore a red turban, her bride rode a chariot. The marriage took place in the presence of their families, many classmates and colleagues of Manjit were present at the ceremony.\n\nText:\nNita Ambani has been made a member of International Olympic Committee's Olympic Channel Commission, which handles IOC's digital platform. Besides the Olympic Channel, she has also been made a member of IOC's Olympic Education Commission. In August last year, Nita became IOC's first Indian woman member after being elected at the 129th IOC Session in Rio de Janeiro.\nSummary:\nEarlier this month, the government announced that it will not be taking any further steps to improve the quality of life of people in India. \"It's important for us to know that we are doing well,\" said an official.\n\nText:\nA 3-foot-long rabbit named Simon, which was expected to become the world's biggest rabbit, died in the cargo section of a United Airlines flight recently travelling from London to Chicago. \"Something very strange has happened...I've sent rabbits all around the world and nothing like this has happened before,\" stated Simon's breeder. The airlines stated that it is reviewing the incident.\nSummary:\nthree-foot-long Simon, destined to be the world's biggest bunny, died mysteriously on a United Airlines flight to the US. The plane was carrying passengers from Heathrow to a new celebrity owner in the US. \"Something very strange has happened and I want to know what. Ive sent rabbits all around the world and nothing like this has happened before,\" said a breeder.\n\nText:\nThe Bharatiya Janata Party on Wednesday swept the Municipal Corporation of Delhi (MCD) elections, winning 181 out of the total 270 wards of the civic body. While the Aam Aadmi Party (AAP) finished second by managing to win 48 wards, the Congress ended up gaining victory in 30 wards. The remaining 11 wards were won by others, including independent candidates.\nSummary:\nBJP won 183, Congress 36 and AAP 41 seats in the MCD elections while rejecting both the Aam Aadmi Party and Congress. The main opposition took 10 wards of the three civic bodies.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}